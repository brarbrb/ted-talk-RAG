{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42ca4fdb",
   "metadata": {},
   "source": [
    "# *Imports*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2574a87c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, json, math, time, hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1d029a",
   "metadata": {},
   "source": [
    "# *Configurations*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b5181ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://api.llmod.ai/v1\"  \n",
    "EMBED_MODEL = \"RPRTHPB-text-embedding-3-small\"\n",
    "EMBED_DIMS = 1536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d02a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking restrictions\n",
    "MAX_TOKENS = 1024\n",
    "MAX_OVERLAP_RATIO = 0.2\n",
    "EMBED_DIMS = 1536\n",
    "\n",
    "# Heuristic since we may not have a tokenizer:\n",
    "# rough token ≈ word * 1.3, so max_words ~ MAX_TOKENS / 1.3\n",
    "MAX_WORDS = int(MAX_TOKENS / 1.3)\n",
    "OVERLAP_WORDS = int(MAX_WORDS * MAX_OVERLAP_RATIO)\n",
    "\n",
    "START_WITH_N_TALKS = 5  # set None to do all data\n",
    "\n",
    "# Output artifact files (cached embeddings)\n",
    "OUT_META_JSONL = \"ted_chunks_meta.jsonl\"\n",
    "OUT_EMB_NPY = \"ted_chunks_embeds.npy\"\n",
    "OUT_IDMAP_JSON = \"ted_chunks_idmap.json\"  # to avoid duplicates across runs\n",
    "\n",
    "# API keys\n",
    "LLMOD_API_KEY = os.getenv(\"LLMOD_API_KEY\")  # or paste here: \"sk-....\"\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "HEADERS = {\"Authorization\": f\"Bearer {LLMOD_API_KEY}\", \"Content-Type\": \"application/json\"}\n",
    "INDEX_NAME = \"ted\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b25833",
   "metadata": {},
   "source": [
    "# *Chunks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2242c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_hash(text: str) -> str:\n",
    "    return hashlib.sha256(text.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def approx_word_chunks(text: str, max_words: int, overlap_words: int):\n",
    "    \"\"\"\n",
    "    Word-based chunker to approximate token limits.\n",
    "    Ensures overlap <= 30% by construction if overlap_words <= 0.3*max_words.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    if not words:\n",
    "        return []\n",
    "\n",
    "    chunks = []\n",
    "    step = max_words - overlap_words\n",
    "    if step <= 0:\n",
    "        raise ValueError(\"overlap_words too large; step must be > 0.\")\n",
    "\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = min(start + max_words, len(words))\n",
    "        chunk_words = words[start:end]\n",
    "        chunk_text = \" \".join(chunk_words).strip()\n",
    "        if chunk_text:\n",
    "            chunks.append(chunk_text)\n",
    "        if end == len(words):\n",
    "            break\n",
    "        start += step\n",
    "    return chunks\n",
    "\n",
    "def embed_texts_batch(texts, model=EMBED_MODEL, dims=EMBED_DIMS, max_retries=6):\n",
    "    \"\"\"\n",
    "    Calls llmod.ai embeddings endpoint (OpenAI-compatible).\n",
    "    Uses exponential backoff on transient errors.\n",
    "    \"\"\"\n",
    "    url = f\"{BASE_URL}/embeddings\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"input\": texts,\n",
    "        \"dimensions\": dims,  # keep aligned with the model default (1536)\n",
    "    }\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            r = requests.post(url, headers=HEADERS, data=json.dumps(payload), timeout=60)\n",
    "            if r.status_code == 200:\n",
    "                data = r.json()\n",
    "                # OpenAI-style: data[\"data\"] is list of {embedding: [...]}\n",
    "                embs = [np.array(item[\"embedding\"], dtype=np.float32) for item in data[\"data\"]]\n",
    "                return np.vstack(embs)\n",
    "            # Retry on rate limit / transient server issues\n",
    "            if r.status_code in (429, 500, 502, 503, 504):\n",
    "                sleep_s = min(2 ** attempt, 30)\n",
    "                time.sleep(sleep_s)\n",
    "                continue\n",
    "            # Otherwise: hard fail with details\n",
    "            raise RuntimeError(f\"Embeddings error {r.status_code}: {r.text[:500]}\")\n",
    "        except requests.RequestException as e:\n",
    "            sleep_s = min(2 ** attempt, 30)\n",
    "            time.sleep(sleep_s)\n",
    "            last_err = e\n",
    "    raise RuntimeError(f\"Embeddings failed after retries. Last error: {last_err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f92b41a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['talk_id', 'title', 'speaker_1', 'all_speakers', 'occupations',\n",
       "       'about_speakers', 'views', 'recorded_date', 'published_date', 'event',\n",
       "       'native_lang', 'available_lang', 'comments', 'duration', 'topics',\n",
       "       'related_talks', 'url', 'description', 'transcript'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Load the dataset CSV\n",
    "# If your dataset is large, consider reading only needed columns first.\n",
    "# Example assumes you already have it locally as ted.csv:\n",
    "CSV_PATH = \"ted_talks_en.csv\"  # <-- change this\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Optional: start small for budget\n",
    "if START_WITH_N_TALKS is not None:\n",
    "    df = df.head(START_WITH_N_TALKS).copy()\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34574701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, dict_keys(['chunk_uid', 'talk_id', 'title', 'chunk_id', 'text']))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) Build chunk records (metadata + text)\n",
    "records = []\n",
    "for _, row in df.iterrows():\n",
    "    talk_id = str(row.get(\"talk_id\", \"\"))\n",
    "    title = str(row.get(\"title\", \"\"))\n",
    "    transcript = str(row.get(\"transcript\", \"\") or \"\")\n",
    "    if not transcript.strip():\n",
    "        continue\n",
    "\n",
    "    chunks = approx_word_chunks(transcript, max_words=MAX_WORDS, overlap_words=OVERLAP_WORDS)\n",
    "\n",
    "    for ci, chunk_text in enumerate(chunks):\n",
    "        chunk_uid = stable_hash(talk_id + \"|\" + title + \"|\" + str(ci) + \"|\" + chunk_text)\n",
    "        records.append({\n",
    "            \"chunk_uid\": chunk_uid,\n",
    "            \"talk_id\": talk_id,\n",
    "            \"title\": title,\n",
    "            \"chunk_id\": ci,\n",
    "            \"text\": chunk_text,\n",
    "        })\n",
    "\n",
    "len(records), records[0].keys() if records else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ecd023d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 14\n",
      "Already embedded: 14\n",
      "To embed now: 0\n"
     ]
    }
   ],
   "source": [
    "# 3) Load previous cache (so we don't re-embed)\n",
    "if os.path.exists(OUT_IDMAP_JSON):\n",
    "    with open(OUT_IDMAP_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "        seen = set(json.load(f))\n",
    "else:\n",
    "    seen = set()\n",
    "\n",
    "new_records = [r for r in records if r[\"chunk_uid\"] not in seen]\n",
    "print(\"Total records:\", len(records))\n",
    "print(\"Already embedded:\", len(records) - len(new_records))\n",
    "print(\"To embed now:\", len(new_records))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6439a3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Embeddings shape: (14, 1536)\n",
      "Saved: ted_chunks_embeds.npy ted_chunks_meta.jsonl ted_chunks_idmap.json\n"
     ]
    }
   ],
   "source": [
    "# 4) Embed new chunks in batches + append to cache\n",
    "BATCH_SIZE = 64  # you can tune; smaller batches can be safer\n",
    "\n",
    "# Load existing embeddings if present\n",
    "if os.path.exists(OUT_EMB_NPY):\n",
    "    old_embs = np.load(OUT_EMB_NPY)\n",
    "else:\n",
    "    old_embs = None\n",
    "\n",
    "new_emb_list = []\n",
    "to_write_meta = []\n",
    "\n",
    "for i in range(0, len(new_records), BATCH_SIZE):\n",
    "    batch = new_records[i:i+BATCH_SIZE]\n",
    "    texts = [b[\"text\"] for b in batch]\n",
    "    embs = embed_texts_batch(texts)\n",
    "\n",
    "    # Sanity check dimensions\n",
    "    if embs.shape[1] != EMBED_DIMS:\n",
    "        raise ValueError(f\"Unexpected embedding dims: {embs.shape[1]} (expected {EMBED_DIMS})\")\n",
    "\n",
    "    new_emb_list.append(embs)\n",
    "    to_write_meta.extend(batch)\n",
    "\n",
    "    print(f\"Embedded {min(i+BATCH_SIZE, len(new_records))}/{len(new_records)}\")\n",
    "\n",
    "# Append embeddings\n",
    "if new_emb_list:\n",
    "    new_embs = np.vstack(new_emb_list)\n",
    "    all_embs = new_embs if old_embs is None else np.vstack([old_embs, new_embs])\n",
    "else:\n",
    "    all_embs = old_embs if old_embs is not None else np.zeros((0, EMBED_DIMS), dtype=np.float32)\n",
    "\n",
    "# Save embeddings matrix\n",
    "np.save(OUT_EMB_NPY, all_embs)\n",
    "\n",
    "# Append metadata (jsonl)\n",
    "if to_write_meta:\n",
    "    with open(OUT_META_JSONL, \"a\", encoding=\"utf-8\") as f:\n",
    "        for r in to_write_meta:\n",
    "            f.write(json.dumps({\n",
    "                \"chunk_uid\": r[\"chunk_uid\"],\n",
    "                \"talk_id\": r[\"talk_id\"],\n",
    "                \"title\": r[\"title\"],\n",
    "                \"chunk_id\": r[\"chunk_id\"],\n",
    "                \"text\": r[\"text\"],\n",
    "            }, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# Update seen set and save\n",
    "for r in to_write_meta:\n",
    "    seen.add(r[\"chunk_uid\"])\n",
    "with open(OUT_IDMAP_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(sorted(list(seen)), f)\n",
    "\n",
    "print(\"Done.\")\n",
    "print(\"Embeddings shape:\", all_embs.shape)\n",
    "print(\"Saved:\", OUT_EMB_NPY, OUT_META_JSONL, OUT_IDMAP_JSON)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834d3627",
   "metadata": {},
   "source": [
    "# Pinecone Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b53417c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready: ted\n"
     ]
    }
   ],
   "source": [
    "existing = [idx[\"name\"] for idx in pc.list_indexes()]\n",
    "\n",
    "if INDEX_NAME not in existing:\n",
    "    pc.create_index(\n",
    "        name=INDEX_NAME,\n",
    "        dimension=EMBED_DIMS, # Ensuring using the correct dimensions (as returned by embedding model)\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "index = pc.Index(INDEX_NAME)\n",
    "print(\"Ready:\", INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b067294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded embeddings: (14, 1536)\n",
      "Loaded meta rows: 14\n",
      "Already upserted IDs (local cache): 14\n",
      "To upsert now: 0\n",
      "✅ Upsert complete.\n",
      "Local upsert cache saved to: pinecone_upserted_ids.json\n",
      "Index stats: {'_response_info': {'raw_headers': {'connection': 'keep-alive',\n",
      "                                    'content-length': '184',\n",
      "                                    'content-type': 'application/json',\n",
      "                                    'date': 'Sat, 27 Dec 2025 13:52:06 GMT',\n",
      "                                    'grpc-status': '0',\n",
      "                                    'server': 'envoy',\n",
      "                                    'x-envoy-upstream-service-time': '93',\n",
      "                                    'x-pinecone-request-id': '538027401554865195',\n",
      "                                    'x-pinecone-request-latency-ms': '92',\n",
      "                                    'x-pinecone-response-duration-ms': '94'}},\n",
      " 'dimension': 1536,\n",
      " 'index_fullness': 0.0,\n",
      " 'memoryFullness': 0.0,\n",
      " 'metric': 'cosine',\n",
      " 'namespaces': {'__default__': {'vector_count': 14}},\n",
      " 'storageFullness': 0.0,\n",
      " 'total_vector_count': 14,\n",
      " 'vector_type': 'dense'}\n"
     ]
    }
   ],
   "source": [
    "# ====== CONTINUE HERE: Upsert cached vectors into Pinecone ======\n",
    "\n",
    "OUT_PINECONE_IDMAP_JSON = \"pinecone_upserted_ids.json\"\n",
    "\n",
    "# Load cached embeddings\n",
    "if not os.path.exists(OUT_EMB_NPY):\n",
    "    raise FileNotFoundError(f\"Missing embeddings file: {OUT_EMB_NPY}\")\n",
    "embs = np.load(OUT_EMB_NPY)\n",
    "\n",
    "# Load cached metadata jsonl (must be same order as embeddings were appended)\n",
    "if not os.path.exists(OUT_META_JSONL):\n",
    "    raise FileNotFoundError(f\"Missing metadata file: {OUT_META_JSONL}\")\n",
    "\n",
    "meta_rows = []\n",
    "with open(OUT_META_JSONL, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            meta_rows.append(json.loads(line))\n",
    "\n",
    "if len(meta_rows) != embs.shape[0]:\n",
    "    raise ValueError(\n",
    "        f\"Mismatch: meta rows={len(meta_rows)} vs embeddings={embs.shape[0]}. \"\n",
    "        \"These must be aligned (same append order).\"\n",
    "    )\n",
    "\n",
    "print(\"Loaded embeddings:\", embs.shape)\n",
    "print(\"Loaded meta rows:\", len(meta_rows))\n",
    "\n",
    "# Load already-upserted ids (local cache to avoid re-upserting)\n",
    "if os.path.exists(OUT_PINECONE_IDMAP_JSON):\n",
    "    with open(OUT_PINECONE_IDMAP_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "        upserted = set(json.load(f))\n",
    "else:\n",
    "    upserted = set()\n",
    "\n",
    "print(\"Already upserted IDs (local cache):\", len(upserted))\n",
    "\n",
    "def safe_metadata(m: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Pinecone metadata must be JSON-serializable, typically simple types.\n",
    "    Also keep text size reasonable (metadata size limits exist).\n",
    "    \"\"\"\n",
    "    text = m.get(\"text\", \"\") or \"\"\n",
    "    # Keep a snippet to avoid oversized metadata; adjust if you want.\n",
    "    text_snippet = text[:2000]\n",
    "\n",
    "    return {\n",
    "        \"talk_id\": str(m.get(\"talk_id\", \"\")),\n",
    "        \"title\": str(m.get(\"title\", \"\")),\n",
    "        \"chunk_id\": int(m.get(\"chunk_id\", 0)),\n",
    "        \"text\": text_snippet,\n",
    "    }\n",
    "\n",
    "# Build items to upsert (skip already upserted IDs)\n",
    "items = []\n",
    "for i, m in enumerate(meta_rows):\n",
    "    _id = m[\"chunk_uid\"]\n",
    "    if _id in upserted:\n",
    "        continue\n",
    "\n",
    "    vec = embs[i]\n",
    "    if vec.shape[0] != EMBED_DIMS:\n",
    "        raise ValueError(f\"Bad dims at row {i}: got {vec.shape[0]} expected {EMBED_DIMS}\")\n",
    "\n",
    "    items.append((_id, vec.tolist(), safe_metadata(m)))\n",
    "\n",
    "print(\"To upsert now:\", len(items))\n",
    "\n",
    "# Upsert in batches\n",
    "UPSERT_BATCH = 100  # safe default; you can set 50 if you prefer\n",
    "for start in range(0, len(items), UPSERT_BATCH):\n",
    "    batch = items[start:start+UPSERT_BATCH]\n",
    "    index.upsert(vectors=batch)\n",
    "\n",
    "    # update local cache\n",
    "    for _id, _, _ in batch:\n",
    "        upserted.add(_id)\n",
    "\n",
    "    if (start // UPSERT_BATCH) % 5 == 0:\n",
    "        print(f\"Upserted {min(start+UPSERT_BATCH, len(items))}/{len(items)}\")\n",
    "\n",
    "# Persist local upsert cache\n",
    "with open(OUT_PINECONE_IDMAP_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(sorted(list(upserted)), f)\n",
    "\n",
    "print(\"✅ Upsert complete.\")\n",
    "print(\"Local upsert cache saved to:\", OUT_PINECONE_IDMAP_JSON)\n",
    "\n",
    "# Optional: quick sanity check by describing index stats\n",
    "try:\n",
    "    stats = index.describe_index_stats()\n",
    "    print(\"Index stats:\", stats)\n",
    "except Exception as e:\n",
    "    print(\"describe_index_stats failed (non-fatal):\", repr(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac08ef9a",
   "metadata": {},
   "source": [
    "# Rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9e4ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== RAG PIPELINE (Query -> Retrieve -> Answer) ======\n",
    "\n",
    "import textwrap\n",
    "\n",
    "CHAT_MODEL = \"RPRTHPB-gpt-5-mini\"\n",
    "\n",
    "# Must choose & report (per assignment)\n",
    "RAG_CHUNK_SIZE_TOKENS = MAX_TOKENS          # 2048 (max allowed)\n",
    "RAG_OVERLAP_RATIO = MAX_OVERLAP_RATIO       # 0.30 (max allowed)\n",
    "RAG_TOP_K = 2                               # <= 30 (tune: 5-12 often good)\n",
    "\n",
    "assert RAG_CHUNK_SIZE_TOKENS <= 2048\n",
    "assert RAG_OVERLAP_RATIO <= 0.30\n",
    "assert 1 <= RAG_TOP_K <= 30\n",
    "\n",
    "RETRIEVE_INCLUDE_TEXT = True  # we stored snippet in metadata[\"text\"]\n",
    "\n",
    "REQUIRED_SYSTEM_PROMPT = \"\"\"You are a TED Talk assistant that answers questions strictly and\n",
    "only based on the TED dataset context provided to you (metadata\n",
    "and transcript passages). You must not use any external\n",
    "knowledge, the open internet, or information that is not explicitly\n",
    "contained in the retrieved context. If the answer cannot be\n",
    "determined from the provided context, respond: “I don’t know\n",
    "based on the provided TED data.” Always explain your answer\n",
    "using the given context, quoting or paraphrasing the relevant\n",
    "transcript or metadata when helpful.\n",
    "\"\"\"\n",
    "\n",
    "def embed_query(text: str) -> np.ndarray:\n",
    "    \"\"\"Embed one query text -> (1536,) float32\"\"\"\n",
    "    emb = embed_texts_batch([text], model=EMBED_MODEL, dims=EMBED_DIMS)\n",
    "    if emb.shape != (1, EMBED_DIMS):\n",
    "        raise ValueError(f\"Bad query embedding shape: {emb.shape}\")\n",
    "    return emb[0].astype(np.float32)\n",
    "\n",
    "def retrieve_from_pinecone(query: str, top_k: int = RAG_TOP_K):\n",
    "    \"\"\"\n",
    "    Returns list of matches with fields: id, score, metadata.\n",
    "    \"\"\"\n",
    "    qvec = embed_query(query)\n",
    "    res = index.query(\n",
    "        vector=qvec.tolist(),\n",
    "        top_k=top_k,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    # Pinecone returns an object/dict-like; normalize:\n",
    "    matches = res.get(\"matches\", res[\"matches\"]) if isinstance(res, dict) else res.matches\n",
    "    # print(\"Returned from PINECONE: \\n\", matches)\n",
    "    out = []\n",
    "    for m in matches:\n",
    "        # handle dict-like or object-like\n",
    "        mid = m.get(\"id\", None) if isinstance(m, dict) else m.id\n",
    "        score = m.get(\"score\", None) if isinstance(m, dict) else m.score\n",
    "        meta = m.get(\"metadata\", {}) if isinstance(m, dict) else (m.metadata or {})\n",
    "        out.append({\"id\": mid, \"score\": float(score) if score is not None else None, \"metadata\": meta})\n",
    "    return out\n",
    "\n",
    "def build_context(matches, max_chars: int = 12000) -> str:\n",
    "    \"\"\"\n",
    "    Build a compact context block from retrieved matches.\n",
    "    We cap total chars to avoid bloating the model context (efficiency).\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    total = 0\n",
    "\n",
    "    for rank, item in enumerate(matches, start=1):\n",
    "        md = item[\"metadata\"] or {}\n",
    "        title = str(md.get(\"title\", \"\"))\n",
    "        talk_id = str(md.get(\"talk_id\", \"\"))\n",
    "        chunk_id = md.get(\"chunk_id\", \"\")\n",
    "        text = str(md.get(\"text\", \"\")) if RETRIEVE_INCLUDE_TEXT else \"\"\n",
    "\n",
    "        block = (\n",
    "            f\"[{rank}] talk_id={talk_id} | title={title} | chunk_id={chunk_id} | score={item['score']:.4f}\\n\"\n",
    "            f\"PASSAGE:\\n{text}\\n\"\n",
    "        )\n",
    "\n",
    "        if total + len(block) > max_chars:\n",
    "            break\n",
    "        parts.append(block)\n",
    "        total += len(block)\n",
    "\n",
    "    return \"\\n---\\n\".join(parts).strip()\n",
    "\n",
    "def print_retrieved(matches, text_chars: int = 400):\n",
    "    print(\"\\n=== RETRIEVED MATCHES ===\")\n",
    "    for i, m in enumerate(matches, start=1):\n",
    "        md = m.get(\"metadata\", {}) or {}\n",
    "        title = md.get(\"title\", \"\")\n",
    "        talk_id = md.get(\"talk_id\", \"\")\n",
    "        chunk_id = md.get(\"chunk_id\", \"\")\n",
    "        score = m.get(\"score\", None)\n",
    "\n",
    "        passage = (md.get(\"text\", \"\") or \"\")\n",
    "        passage = passage[:text_chars] + (\"...\" if len(passage) > text_chars else \"\")\n",
    "\n",
    "        print(f\"\\n[{i}] score={score:.4f} | talk_id={talk_id} | chunk_id={chunk_id} | title={title}\")\n",
    "        print(\"PASSAGE:\", passage)\n",
    "\n",
    "\n",
    "def call_chat_model(question: str, context: str) -> str:\n",
    "    \"\"\"\n",
    "    Calls llmod.ai chat completions endpoint (OpenAI-compatible).\n",
    "    \"\"\"\n",
    "    url = f\"{BASE_URL}/chat/completions\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": REQUIRED_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": f\"TED DATA CONTEXT:\\n{context}\\n\\nQUESTION:\\n{question}\"},\n",
    "    ]\n",
    "    payload = {\n",
    "        \"model\": CHAT_MODEL,\n",
    "        \"messages\": messages,\n",
    "    }\n",
    "\n",
    "    # minimal retry/backoff\n",
    "    max_retries = 6\n",
    "    last_err = None\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            r = requests.post(url, headers=HEADERS, data=json.dumps(payload), timeout=90)\n",
    "            if r.status_code == 200:\n",
    "                data = r.json()\n",
    "                return data[\"choices\"][0][\"message\"][\"content\"]\n",
    "            if r.status_code in (429, 500, 502, 503, 504):\n",
    "                time.sleep(min(2 ** attempt, 30))\n",
    "                continue\n",
    "            raise RuntimeError(f\"Chat error {r.status_code}: {r.text[:500]}\")\n",
    "        except requests.RequestException as e:\n",
    "            last_err = e\n",
    "            time.sleep(min(2 ** attempt, 30))\n",
    "    raise RuntimeError(f\"Chat failed after retries. Last error: {last_err}\")\n",
    "\n",
    "def rag_answer(question: str, top_k: int = RAG_TOP_K, min_matches: int = 1, debug: bool = True):\n",
    "    \"\"\"\n",
    "    End-to-end RAG: retrieve -> build context -> ask model.\n",
    "    \"\"\"\n",
    "    matches = retrieve_from_pinecone(question, top_k=top_k)\n",
    "    if debug:\n",
    "        print(f\"Retrieved: {len(matches)} matches (top_k={top_k})\")\n",
    "        print_retrieved(matches, text_chars=600)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Retrieved: {len(matches)} matches (top_k={top_k})\")\n",
    "\n",
    "    if len(matches) < min_matches:\n",
    "        return \"I don’t know based on the provided TED data.\"\n",
    "\n",
    "    context = build_context(matches)\n",
    "    if debug:\n",
    "        print(\"\\nContext preview (first 600 chars):\")\n",
    "        print(context[:600] + (\"...\" if len(context) > 600 else \"\"))\n",
    "        print(\"\\n---\\nRAG hyperparameters to report:\")\n",
    "        print(f\"chunk_size_tokens={RAG_CHUNK_SIZE_TOKENS}, overlap_ratio={RAG_OVERLAP_RATIO}, top_k={top_k}\")\n",
    "\n",
    "    answer = call_chat_model(question, context)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9ed638a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returned from PINECONE: \n",
      " [{'id': '8fba17446ff938c960e0cdb07c4541dc19a74e071827f48e8dcc1b5b0a729aa5',\n",
      " 'metadata': {'chunk_id': 0,\n",
      "              'talk_id': '66',\n",
      "              'text': \"Good morning. How are you? (Audience) Good. It's been \"\n",
      "                      \"great, hasn't it? I've been blown away by the whole \"\n",
      "                      \"thing. In fact, I'm leaving. (Laughter) There have been \"\n",
      "                      'three themes running through the conference, which are '\n",
      "                      'relevant to what I want to talk about. One is the '\n",
      "                      'extraordinary evidence of human creativity in all of '\n",
      "                      \"the presentations that we've had and in all of the \"\n",
      "                      'people here; just the variety of it and the range of '\n",
      "                      \"it. The second is that it's put us in a place where we \"\n",
      "                      \"have no idea what's going to happen in terms of the \"\n",
      "                      'future. No idea how this may play out. I have an '\n",
      "                      'interest in education. Actually, what I find is, '\n",
      "                      \"everybody has an interest in education. Don't you? I \"\n",
      "                      \"find this very interesting. If you're at a dinner \"\n",
      "                      'party, and you say you work in education — actually, '\n",
      "                      \"you're not often at dinner parties, frankly. (Laughter) \"\n",
      "                      \"If you work in education, you're not asked. (Laughter) \"\n",
      "                      \"And you're never asked back, curiously. That's strange \"\n",
      "                      'to me. But if you are, and you say to somebody, you '\n",
      "                      'know, they say, \"What do you do?\" and you say you work '\n",
      "                      'in education, you can see the blood run from their '\n",
      "                      'face. They\\'re like, \"Oh my God. Why me?\" (Laughter) '\n",
      "                      '\"My one night out all week.\" (Laughter) But if you ask '\n",
      "                      'about their education, they pin you to the wall, '\n",
      "                      \"because it's one of those things that goes deep with \"\n",
      "                      'people, am I right? Like religion and money and other '\n",
      "                      'things. So I have a big interest in education, and I '\n",
      "                      'think we all do. We have a huge vested interest in it, '\n",
      "                      \"partly because it's education that's meant to take us \"\n",
      "                      \"into this future that we can't grasp. If you think of \"\n",
      "                      'it, children starting school this year will be retiring '\n",
      "                      'in 2065. Nobody has a clue, despite all the expertise '\n",
      "                      \"that's been on parade for the past four days, what the \"\n",
      "                      \"world will look like in five years' time. And yet, \"\n",
      "                      \"we're meant to be educating them for it. So the \"\n",
      "                      'unpredictability, I think, is extraordinary. And the '\n",
      "                      \"third part of this is that we've all agreed, \"\n",
      "                      'nonetheless, on the really extraordinary capacitie',\n",
      "              'title': 'Do schools kill creativity?'},\n",
      " 'score': 0.557052672,\n",
      " 'values': []}, {'id': 'a8f2705ee05d741dfa4201ec4804c8571eeec29eda0f3144fc4dfdf44537d538',\n",
      " 'metadata': {'chunk_id': 2,\n",
      "              'talk_id': '66',\n",
      "              'text': 'off from Helen yesterday, this is probably why women '\n",
      "                      \"are better at multitasking. Because you are, aren't \"\n",
      "                      \"you? There's a raft of research, but I know it from my \"\n",
      "                      'personal life. If my wife is cooking a meal at home, '\n",
      "                      \"which is not often ... thankfully. (Laughter) No, she's \"\n",
      "                      \"good at some things. But if she's cooking, she's \"\n",
      "                      \"dealing with people on the phone, she's talking to the \"\n",
      "                      \"kids, she's painting the ceiling — (Laughter) she's \"\n",
      "                      \"doing open-heart surgery over here. If I'm cooking, the \"\n",
      "                      \"door is shut, the kids are out, the phone's on the \"\n",
      "                      'hook, if she comes in, I get annoyed. I say, \"Terry, '\n",
      "                      'please, I\\'m trying to fry an egg in here.\" (Laughter) '\n",
      "                      '\"Give me a break.\" (Laughter) Actually, do you know '\n",
      "                      'that old philosophical thing, \"If a tree falls in a '\n",
      "                      'forest, and nobody hears it, did it happen?\" Remember '\n",
      "                      'that old chestnut? I saw a great T-shirt recently, '\n",
      "                      'which said, \"If a man speaks his mind in a forest, and '\n",
      "                      'no woman hears him, is he still wrong?\" (Laughter) And '\n",
      "                      \"the third thing about intelligence is, it's distinct. \"\n",
      "                      'I\\'m doing a new book at the moment called \"Epiphany,\" '\n",
      "                      'which is based on a series of interviews with people '\n",
      "                      \"about how they discovered their talent. I'm fascinated \"\n",
      "                      \"by how people got to be there. It's really prompted by \"\n",
      "                      'a conversation I had with a wonderful woman who maybe '\n",
      "                      'most people have never heard of, Gillian Lynne. Have '\n",
      "                      \"you heard of her? Some have. She's a choreographer, and \"\n",
      "                      'everybody knows her work. She did \"Cats\" and \"Phantom '\n",
      "                      'of the Opera.\" She\\'s wonderful. I used to be on the '\n",
      "                      'board of The Royal Ballet, as you can see. (Laughter) '\n",
      "                      'Gillian and I had lunch one day. I said, \"How did you '\n",
      "                      'get to be a dancer?\" It was interesting. When she was '\n",
      "                      'at school, she was really hopeless. And the school, in '\n",
      "                      'the \\'30s, wrote to her parents and said, \"We think '\n",
      "                      'Gillian has a learning disorder.\" She couldn\\'t '\n",
      "                      \"concentrate; she was fidgeting. I think now they'd say \"\n",
      "                      \"she had ADHD. Wouldn't you? But this was the 1930s, and \"\n",
      "                      \"ADHD hadn't been invented at this point. It wasn't an \"\n",
      "                      'available condition. (',\n",
      "              'title': 'Do schools kill creativity?'},\n",
      " 'score': 0.543563902,\n",
      " 'values': []}]\n",
      "Retrieved: 2 matches (top_k=2)\n",
      "\n",
      "=== RETRIEVED MATCHES ===\n",
      "\n",
      "[1] score=0.5571 | talk_id=66 | chunk_id=0 | title=Do schools kill creativity?\n",
      "PASSAGE: Good morning. How are you? (Audience) Good. It's been great, hasn't it? I've been blown away by the whole thing. In fact, I'm leaving. (Laughter) There have been three themes running through the conference, which are relevant to what I want to talk about. One is the extraordinary evidence of human creativity in all of the presentations that we've had and in all of the people here; just the variety of it and the range of it. The second is that it's put us in a place where we have no idea what's going to happen in terms of the future. No idea how this may play out. I have an interest in educatio...\n",
      "\n",
      "[2] score=0.5436 | talk_id=66 | chunk_id=2 | title=Do schools kill creativity?\n",
      "PASSAGE: off from Helen yesterday, this is probably why women are better at multitasking. Because you are, aren't you? There's a raft of research, but I know it from my personal life. If my wife is cooking a meal at home, which is not often ... thankfully. (Laughter) No, she's good at some things. But if she's cooking, she's dealing with people on the phone, she's talking to the kids, she's painting the ceiling — (Laughter) she's doing open-heart surgery over here. If I'm cooking, the door is shut, the kids are out, the phone's on the hook, if she comes in, I get annoyed. I say, \"Terry, please, I'm try...\n",
      "Retrieved: 2 matches (top_k=2)\n",
      "\n",
      "Context preview (first 600 chars):\n",
      "[1] talk_id=66 | title=Do schools kill creativity? | chunk_id=0 | score=0.5571\n",
      "PASSAGE:\n",
      "Good morning. How are you? (Audience) Good. It's been great, hasn't it? I've been blown away by the whole thing. In fact, I'm leaving. (Laughter) There have been three themes running through the conference, which are relevant to what I want to talk about. One is the extraordinary evidence of human creativity in all of the presentations that we've had and in all of the people here; just the variety of it and the range of it. The second is that it's put us in a place where we have no idea what's going to happ...\n",
      "\n",
      "---\n",
      "RAG hyperparameters to report:\n",
      "chunk_size_tokens=2048, overlap_ratio=0.3, top_k=2\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# ====== QUICK TEST ======\n",
    "q = \"What is the main message of the talk about education and creativity?\"\n",
    "print(rag_answer(q, top_k=RAG_TOP_K, debug=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
